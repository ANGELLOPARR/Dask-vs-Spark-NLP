{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa234de-37d3-4e92-bb1c-5f32239d025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x11ff2f400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# MISC\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from shared import *\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "data_path_50MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Gift_Cards.json'\n",
    "data_path_263MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Appliances.json'\n",
    "data_path_755MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Digital_Music.json'\n",
    "data_path_2GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Video_Games.json'\n",
    "data_path_10GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/reviews_Books_1.json'\n",
    "data_path_38GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Books_big.json'\n",
    "\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask import dataframe as dd\n",
    "import dask.bag as db\n",
    "dask.config.set(scheduler='processes', num_workers=cpu_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889a3ac-89ee-44c1-a2f7-fffd6bc79923",
   "metadata": {},
   "source": [
    "## Dask Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d75f4ea-ff49-44fe-90d0-c02f088f73b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for operation: 0.13904285430908203\n",
      "Elapsed time for operation: 0.0007040500640869141\n",
      "Elapsed time for operation: 0.0018210411071777344\n",
      "Elapsed time for operation: 0.0014040470123291016\n",
      "Elapsed time for operation: 0.0013570785522460938\n",
      "Elapsed time for operation: 0.0017154216766357422\n",
      "Elapsed time for operation: 0.001486063003540039\n",
      "Total execution time: 14.231302976608276\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "start = begin\n",
    "\n",
    "reviews_dd = db.read_text(data_path_50MB).map(json.loads).to_dataframe()\n",
    "reviews_dd = reviews_dd.repartition(npartitions=8)\n",
    "\n",
    "print_elapsed(start)\n",
    "\n",
    "## Drop unwanted columns / keep good ones\n",
    "start = time.time()\n",
    "columns = ['reviewerID', 'reviewText']\n",
    "reviews_dd = reviews_dd[columns]\n",
    "print_elapsed(start)\n",
    "\n",
    "## Drop empty (NaN) rows\n",
    "start = time.time()\n",
    "reviews_dd = reviews_dd.dropna()\n",
    "print_elapsed(start)\n",
    "\n",
    "## Tokenize words in review\n",
    "start = time.time()\n",
    "reviews_dd['tokenized_dirty'] = reviews_dd['reviewText'].apply(nltk.word_tokenize, meta=('reviewText', 'object'))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Remove stop words\n",
    "start = time.time()\n",
    "reviews_dd['tokenized_review'] = reviews_dd['tokenized_dirty'].apply(lambda tokens: [w for w in tokens if w.lower() not in stop_words], meta=('tokenized_dirty', 'object'))\n",
    "\n",
    "## Perform Part-Of-Speech (POS) tagging on tokens\n",
    "start = time.time()\n",
    "reviews_dd['tok/pos'] = reviews_dd['tokenized_review'].apply(my_pos_tag, meta=('tok/pos', 'object'))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Stem words in tokenized review\n",
    "start = time.time()\n",
    "reviews_dd['stemmed_review'] = reviews_dd['tokenized_review'].apply(lambda tokens: [stemmer.stem(t) for t in tokens], meta=('tokenized_review', 'object'))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Lemmatize words in tokenized review\n",
    "start = time.time()\n",
    "reviews_dd['lemmatized_review'] = reviews_dd['tok/pos'].apply(lambda tokens: [lemmatizer.lemmatize(t, pos) for (t, pos) in tokens], meta=('tok/pos', 'object'))\n",
    "print_elapsed(start)\n",
    "\n",
    "final_dd = reviews_dd.head(5)\n",
    "\n",
    "print(f'Total execution time: {time.time() - begin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaac35f8-abe5-45d9-a88d-f0bab92d602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>tokenized_dirty</th>\n",
       "      <th>tokenized_review</th>\n",
       "      <th>tok/pos</th>\n",
       "      <th>stemmed_review</th>\n",
       "      <th>lemmatized_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APV13CM0919JD</td>\n",
       "      <td>Amazon,\\nI am shopping for Amazon.com gift car...</td>\n",
       "      <td>[Amazon, ,, I, am, shopping, for, Amazon.com, ...</td>\n",
       "      <td>[Amazon, ,, shopping, Amazon.com, gift, cards,...</td>\n",
       "      <td>[(Amazon, n), (,, n), (shopping, v), (Amazon.c...</td>\n",
       "      <td>[amazon, ,, shop, amazon.com, gift, card, chri...</td>\n",
       "      <td>[Amazon, ,, shop, Amazon.com, gift, card, Chri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3G8U1G1V082SN</td>\n",
       "      <td>I got this gift card from a friend, and it was...</td>\n",
       "      <td>[I, got, this, gift, card, from, a, friend, ,,...</td>\n",
       "      <td>[got, gift, card, friend, ,, best, !, site, mu...</td>\n",
       "      <td>[(got, v), (gift, a), (card, n), (friend, n), ...</td>\n",
       "      <td>[got, gift, card, friend, ,, best, !, site, mu...</td>\n",
       "      <td>[get, gift, card, friend, ,, best, !, site, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A11T2Q0EVTUWP</td>\n",
       "      <td>aren't we going to save trees?! :) People who ...</td>\n",
       "      <td>[are, n't, we, going, to, save, trees, ?, !, :...</td>\n",
       "      <td>[n't, going, save, trees, ?, !, :, ), People, ...</td>\n",
       "      <td>[(n't, r), (going, v), (save, v), (trees, n), ...</td>\n",
       "      <td>[n't, go, save, tree, ?, !, :, ), peopl, compl...</td>\n",
       "      <td>[n't, go, save, tree, ?, !, :, ), People, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A9YKGBH3SV22C</td>\n",
       "      <td>You can always get someone something from Amaz...</td>\n",
       "      <td>[You, can, always, get, someone, something, fr...</td>\n",
       "      <td>[always, get, someone, something, Amazon, safe...</td>\n",
       "      <td>[(always, r), (get, v), (someone, n), (somethi...</td>\n",
       "      <td>[alway, get, someon, someth, amazon, safeti, n...</td>\n",
       "      <td>[always, get, someone, something, Amazon, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A34WZIHVF3OKOL</td>\n",
       "      <td>Why take 50 dollars of good money with no limi...</td>\n",
       "      <td>[Why, take, 50, dollars, of, good, money, with...</td>\n",
       "      <td>[take, 50, dollars, good, money, limitations, ...</td>\n",
       "      <td>[(take, v), (50, n), (dollars, n), (good, a), ...</td>\n",
       "      <td>[take, 50, dollar, good, money, limit, ,, turn...</td>\n",
       "      <td>[take, 50, dollar, good, money, limitation, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                                         reviewText  \\\n",
       "0   APV13CM0919JD  Amazon,\\nI am shopping for Amazon.com gift car...   \n",
       "1  A3G8U1G1V082SN  I got this gift card from a friend, and it was...   \n",
       "2   A11T2Q0EVTUWP  aren't we going to save trees?! :) People who ...   \n",
       "3   A9YKGBH3SV22C  You can always get someone something from Amaz...   \n",
       "4  A34WZIHVF3OKOL  Why take 50 dollars of good money with no limi...   \n",
       "\n",
       "                                     tokenized_dirty  \\\n",
       "0  [Amazon, ,, I, am, shopping, for, Amazon.com, ...   \n",
       "1  [I, got, this, gift, card, from, a, friend, ,,...   \n",
       "2  [are, n't, we, going, to, save, trees, ?, !, :...   \n",
       "3  [You, can, always, get, someone, something, fr...   \n",
       "4  [Why, take, 50, dollars, of, good, money, with...   \n",
       "\n",
       "                                    tokenized_review  \\\n",
       "0  [Amazon, ,, shopping, Amazon.com, gift, cards,...   \n",
       "1  [got, gift, card, friend, ,, best, !, site, mu...   \n",
       "2  [n't, going, save, trees, ?, !, :, ), People, ...   \n",
       "3  [always, get, someone, something, Amazon, safe...   \n",
       "4  [take, 50, dollars, good, money, limitations, ...   \n",
       "\n",
       "                                             tok/pos  \\\n",
       "0  [(Amazon, n), (,, n), (shopping, v), (Amazon.c...   \n",
       "1  [(got, v), (gift, a), (card, n), (friend, n), ...   \n",
       "2  [(n't, r), (going, v), (save, v), (trees, n), ...   \n",
       "3  [(always, r), (get, v), (someone, n), (somethi...   \n",
       "4  [(take, v), (50, n), (dollars, n), (good, a), ...   \n",
       "\n",
       "                                      stemmed_review  \\\n",
       "0  [amazon, ,, shop, amazon.com, gift, card, chri...   \n",
       "1  [got, gift, card, friend, ,, best, !, site, mu...   \n",
       "2  [n't, go, save, tree, ?, !, :, ), peopl, compl...   \n",
       "3  [alway, get, someon, someth, amazon, safeti, n...   \n",
       "4  [take, 50, dollar, good, money, limit, ,, turn...   \n",
       "\n",
       "                                   lemmatized_review  \n",
       "0  [Amazon, ,, shop, Amazon.com, gift, card, Chri...  \n",
       "1  [get, gift, card, friend, ,, best, !, site, mu...  \n",
       "2  [n't, go, save, tree, ?, !, :, ), People, comp...  \n",
       "3  [always, get, someone, something, Amazon, safe...  \n",
       "4  [take, 50, dollar, good, money, limitation, ,,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119690bd-2b3c-49fc-88c8-397bff4762d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
