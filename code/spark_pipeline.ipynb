{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d73b26-1785-416e-b8e4-7be6ece7d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/10 17:07:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/10 17:07:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# MISC\n",
    "import gzip\n",
    "import multiprocessing\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from shared import *\n",
    "cpu_cores = multiprocessing.cpu_count()\n",
    "data_path_50MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Gift_Cards.json'\n",
    "data_path_263MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Appliances.json'\n",
    "data_path_755MB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Digital_Music.json'\n",
    "data_path_2GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Video_Games.json'\n",
    "data_path_10GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/reviews_Books_1.json'\n",
    "data_path_38GB = '/Users/angelloparr/Documents/csc369/project.nosync/data_uncompressed/Books_big.json'\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Spark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5691cc2b-d303-48e4-9dd8-d15907a4f7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for operation: 7.429738998413086\n",
      "Elapsed time for operation: 0.04822134971618652\n",
      "Elapsed time for operation: 0.045671939849853516\n",
      "Elapsed time for operation: 0.06178712844848633\n",
      "Elapsed time for operation: 0.03279399871826172\n",
      "Elapsed time for operation: 0.03348898887634277\n",
      "Elapsed time for operation: 0.039048194885253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    reviewerID|          reviewText|     tokenized_dirty|    tokenized_review|             tok_pos|        stemmed_text|     lemmatized_text|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| APV13CM0919JD|Amazon,\\nI am sho...|[Amazon, ,, I, am...|[Amazon, ,, shopp...|[[Ljava.lang.Obje...|[amazon, ,, shop,...|[Amazon, ,, shop,...|\n",
      "|A3G8U1G1V082SN|I got this gift c...|[I, got, this, gi...|[got, gift, card,...|[[Ljava.lang.Obje...|[got, gift, card,...|[get, gift, card,...|\n",
      "| A11T2Q0EVTUWP|aren't we going t...|[are, n't, we, go...|[n't, going, save...|[[Ljava.lang.Obje...|[n't, go, save, t...|[n't, go, save, t...|\n",
      "| A9YKGBH3SV22C|You can always ge...|[You, can, always...|[always, get, som...|[[Ljava.lang.Obje...|[alway, get, some...|[always, get, som...|\n",
      "|A34WZIHVF3OKOL|Why take 50 dolla...|[Why, take, 50, d...|[take, 50, dollar...|[[Ljava.lang.Obje...|[take, 50, dollar...|[take, 50, dollar...|\n",
      "|A221J8EC5HNPY6|Gift Cards are th...|[Gift, Cards, are...|[Gift, Cards, tru...|[[Ljava.lang.Obje...|[gift, card, trul...|[Gift, Cards, tru...|\n",
      "|A204VFHD6JWASO|I am a dedicated ...|[I, am, a, dedica...|[dedicated, Amazo...|[[Ljava.lang.Obje...|[dedic, amazon.co...|[dedicate, Amazon...|\n",
      "|A3QN3GMNS7NBIQ|Bought three $50 ...|[Bought, three, $...|[Bought, three, $...|[[Ljava.lang.Obje...|[bought, three, $...|[Bought, three, $...|\n",
      "|A1BXZIDZOMBAV2|At this time of y...|[At, this, time, ...|[time, year, Amaz...|[[Ljava.lang.Obje...|[time, year, amaz...|[time, year, Amaz...|\n",
      "| AJ7D8N3Z1Q7MK|I bought 8 gift c...|[I, bought, 8, gi...|[bought, 8, gift,...|[[Ljava.lang.Obje...|[bought, 8, gift,...|[buy, 8, gift, ca...|\n",
      "|A36UACIX4M8IVQ|GREAT when they a...|[GREAT, when, the...|[GREAT, giving, g...|[[Ljava.lang.Obje...| [great, give, gift]| [GREAT, give, gift]|\n",
      "| A5FEAOLPMWJQB|   i like it is good|[i, like, it, is,...|        [like, good]|[[Ljava.lang.Obje...|        [like, good]|        [like, good]|\n",
      "|A14XDZ9HB51T1H|           very good|        [very, good]|              [good]|[[Ljava.lang.Obje...|              [good]|              [good]|\n",
      "|A2UZS69NXZ1WW7|Gift cards are al...|[Gift, cards, are...|[Gift, cards, alw...|[[Ljava.lang.Obje...|[gift, card, alwa...|[Gift, card, alwa...|\n",
      "|A2AB6HL83JZ1BO|It is a gift card...|[It, is, a, gift,...|[gift, card, ., N...|[[Ljava.lang.Obje...|[gift, card, ., n...|[gift, card, ., N...|\n",
      "|A21WZ65DFKQ5N8|     Can't go wrong!|[Ca, n't, go, wro...|[Ca, n't, go, wro...|[[Ljava.lang.Obje...|[ca, n't, go, wro...|[Ca, n't, go, wro...|\n",
      "|A1VN7BM6Y5TQJA|          Excellent!|      [Excellent, !]|      [Excellent, !]|[[Ljava.lang.Obje...|          [excel, !]|      [Excellent, !]|\n",
      "| AHWCK6XEI8511|All you're produc...|[All, you, 're, p...|['re, products, p...|[[Ljava.lang.Obje...|['re, product, pe...|['re, product, pe...|\n",
      "|A2CKVUNA97MJOU|I got a $50 gift ...|[I, got, a, $, 50...|[got, $, 50, gift...|[[Ljava.lang.Obje...|[got, $, 50, gift...|[get, $, 50, gift...|\n",
      "| A7I9KD0PJAEAX|What a great time...|[What, a, great, ...|[great, time, onl...|[[Ljava.lang.Obje...|[great, time, onl...|[great, time, onl...|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total execution time: 10.174787759780884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "start = begin\n",
    "\n",
    "spark_df = spark.read.json(data_path_50MB)\n",
    "print_elapsed(start)\n",
    "\n",
    "## Drop unwanted columns / keep good ones\n",
    "start = time.time()\n",
    "columns = ['reviewerID', 'reviewText']\n",
    "spark_df = spark_df.select(*columns)\n",
    "print_elapsed(start)\n",
    "\n",
    "## Drop empty (NaN) rows\n",
    "start = time.time()\n",
    "spark_df = spark_df.dropna()\n",
    "print_elapsed(start)\n",
    "\n",
    "## Tokenize words in review\n",
    "start = time.time()\n",
    "udf_tokenize = udf(nltk.word_tokenize)\n",
    "spark_df = spark_df.withColumn(\"tokenized_dirty\", udf_tokenize(spark_df.reviewText))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Remove stop words in reviews\n",
    "start = time.time()\n",
    "udf_clean = udf(lambda tokens: [w for w in tokens if w.lower() not in stop_words])\n",
    "spark_df = spark_df.withColumn(\"tokenized_review\", udf_clean(spark_df.tokenized_dirty))\n",
    "\n",
    "## Perform Part-Of-Speech (POS) tagging on tokens\n",
    "start = time.time()\n",
    "udf_pos = udf(my_pos_tag)\n",
    "spark_df = spark_df.withColumn('tok_pos', udf_pos(spark_df.tokenized_review))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Stem words in tokenized review\n",
    "start = time.time()\n",
    "udf_stem = udf(lambda tokens: [stemmer.stem(t) for t in tokens])\n",
    "spark_df = spark_df.withColumn(\"stemmed_text\", udf_stem(spark_df.tokenized_review))\n",
    "print_elapsed(start)\n",
    "\n",
    "## Lemmatize words in tokenized review\n",
    "start = time.time()\n",
    "udf_lemmatize = udf(lambda tokens: [lemmatizer.lemmatize(t, pos) for (t, pos) in tokens])\n",
    "spark_df = spark_df.withColumn(\"lemmatized_text\", udf_lemmatize(spark_df.tok_pos))\n",
    "print_elapsed(start)\n",
    "   \n",
    "final_df = spark_df.show()\n",
    "\n",
    "print(f'Total execution time: {time.time() - begin}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
